#Model definition 
import torch
import torch.nn as nn
import torch.nn.functional as F

class UNet(nn.Module):
    def __init__(self, in_channels=3, out_channels=13):  
        super(UNet, self).__init__()

        # Double Convolution Block - Conv -> ReLU -> Conv -> ReLU
        def double_conv(in_channels, out_channels):
            return nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
                nn.ReLU(inplace=True)
            )

        # Encoder Path (Downsampling)
        self.encoder1 = double_conv(in_channels, 64)    # First encoder block
        self.encoder2 = double_conv(64, 128)             # Second encoder block
        self.encoder3 = double_conv(128, 256)            # Third encoder block
        self.encoder4 = double_conv(256, 512)            # Fourth encoder block

        # MaxPooling layer to downsample
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)

        # Bottleneck (bridge between encoder and decoder)
        self.bottleneck = double_conv(512, 1024)

        # Decoder Path (Upsampling + Skip Connections)
        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)
        self.decoder4 = double_conv(1024, 512)

        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)
        self.decoder3 = double_conv(512, 256)

        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)
        self.decoder2 = double_conv(256, 128)

        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)
        self.decoder1 = double_conv(128, 64)

        # Final Convolution Layer - Maps to 13 channels (one per class)
        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)

    def forward(self, x):
        """
        Forward pass of U-Net.
        Applies encoder, bottleneck, decoder, and final conv to input image.
        """
        # Encoder
        enc1 = self.encoder1(x)
        enc2 = self.encoder2(self.pool(enc1))
        enc3 = self.encoder3(self.pool(enc2))
        enc4 = self.encoder4(self.pool(enc3))

        # Bottleneck
        bottleneck = self.bottleneck(self.pool(enc4))

        # Decoder with Skip Connections
        dec4 = self.upconv4(bottleneck)
        dec4 = torch.cat((dec4, enc4), dim=1)
        dec4 = self.decoder4(dec4)

        dec3 = self.upconv3(dec4)
        dec3 = torch.cat((dec3, enc3), dim=1)
        dec3 = self.decoder3(dec3)

        dec2 = self.upconv2(dec3)
        dec2 = torch.cat((dec2, enc2), dim=1)
        dec2 = self.decoder2(dec2)

        dec1 = self.upconv1(dec2)
        dec1 = torch.cat((dec1, enc1), dim=1)
        dec1 = self.decoder1(dec1)

        return self.final_conv(dec1)  # No sigmoid/softmax here â€” handled by loss function.
#Data Loader
import os
import cv2
import torch
import numpy as np
from torch.utils.data import Dataset

# Mapping of RGB values to class IDs
RGB_TO_CLASS = {
    (80, 80, 80): 0,    # Background
    (17, 17, 17): 1,    # Abdominal Wall
    (33, 33, 33): 2,    # Liver
    (19, 19, 19): 3,    # GI Tract
    (18, 18, 18): 4,    # Fat
    (49, 49, 49): 5,    # Grasper
    (35, 35, 35): 6,    # Connective Tissue
    (36, 36, 36): 7,    # Blood
    (37, 37, 37): 8,    # Cystic Duct
    (50, 50, 50): 9,    # L-Hook
    (34, 34, 34): 10,   # Gallbladder
    (51, 51, 51): 11,   # Hepatic Vein
    (5, 5, 5): 12       # Liver Ligament
}

def rgb_to_class(mask_rgb):
    """Convert (H, W, 3) RGB mask to (H, W) class ID mask."""
    h, w, _ = mask_rgb.shape
    class_mask = np.zeros((h, w), dtype=np.uint8)

    for rgb, class_id in RGB_TO_CLASS.items():
        match = (mask_rgb == rgb).all(axis=-1)
        class_mask[match] = class_id

    return class_mask

class CholecSeg8kDataset(Dataset):
    def __init__(self, data_folder):
        self.data_folder = data_folder
        self.image_files = [f for f in os.listdir(data_folder) if '_image.png' in f]

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        image_file = self.image_files[idx]
        mask_file = image_file.replace('_image.png', '_watershed_mask.png')

        image_path = os.path.join(self.data_folder, image_file)
        mask_path = os.path.join(self.data_folder, mask_file)

        # Check if files exist
        if not os.path.exists(image_path):
            raise FileNotFoundError(f"Image file missing: {image_path}")
        if not os.path.exists(mask_path):
            raise FileNotFoundError(f"Mask file missing: {mask_path}")

        # Load image and mask
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        mask = cv2.imread(mask_path)
        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)
        mask = rgb_to_class(mask)

        # Resize both to (480, 832)
        TARGET_HEIGHT = 480
        TARGET_WIDTH = 832
        image = cv2.resize(image, (TARGET_WIDTH, TARGET_HEIGHT), interpolation=cv2.INTER_LINEAR)
        mask = cv2.resize(mask, (TARGET_WIDTH, TARGET_HEIGHT), interpolation=cv2.INTER_NEAREST)

        # Convert to tensors
        image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1) / 255.0
        mask = torch.tensor(mask, dtype=torch.long)

        return image, mask
#Model Training
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader


train_folder = r"C:\Users\karth\OneDrive\Desktop\Project\split_cholecseg8k\train"
val_folder = r"C:\Users\karth\OneDrive\Desktop\Project\split_cholecseg8k\val"

# Load datasets & DataLoaders 
train_dataset = CholecSeg8kDataset(train_folder)
val_dataset = CholecSeg8kDataset(val_folder)

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=8)

# Device setup to use gpu or cpu
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Initialize U-Net
model = UNet(in_channels=3, out_channels=13).to(device)

# CrossEntropyLoss 
loss_fn = nn.CrossEntropyLoss()

# Optimizer
optimizer = optim.Adam(model.parameters(), lr=1e-4)

# Training parameters
num_epochs = 25
best_val_loss = float("inf")
save_path = "best_unet_multiclass.pth"

# Training Loop 
for epoch in range(num_epochs):
    print(f"\nEpoch {epoch+1}/{num_epochs}")

    # Training phase
    model.train()
    train_loss = 0.0

    for images, masks in train_loader:
        images = images.to(device)                    
        masks = masks.to(device)                      

        outputs = model(images)                       

        loss = loss_fn(outputs, masks)                 

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_loss += loss.item()

    train_loss /= len(train_loader)

    # Validation phase
    model.eval()
    val_loss = 0.0

    with torch.no_grad():
        for images, masks in val_loader:
            images = images.to(device)
            masks = masks.to(device)

            outputs = model(images)  # [B, 13, H, W]
            loss = loss_fn(outputs, masks)

            val_loss += loss.item()

    val_loss /= len(val_loader)

    print(f"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")

    # Save best model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), save_path)
        print(f"Model saved (new best loss: {best_val_loss:.4f})")

print("Training complete!")

