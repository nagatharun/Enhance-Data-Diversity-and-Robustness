{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcea44db-2097-41ae-9f75-bd275d1de15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.103 ðŸš€ Python-3.12.10 torch-2.6.0+cu124 CPU (Intel Xeon Gold 6226R 2.90GHz)\n",
      "YOLO11n-seg summary (fused): 113 layers, 2,834,763 parameters, 0 gradients, 10.2 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/student/vcheruku/Enhance-Data-Diversity-and-Robustness/yolo-for-single-class/YOLO11/yolo-on-single-object-detection/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) ((1, 37, 8400), (1, 32, 160, 160)) (5.7 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.51...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 16.7s, saved as '/student/vcheruku/Enhance-Data-Diversity-and-Robustness/yolo-for-single-class/YOLO11/yolo-on-single-object-detection/weights/best.onnx' (11.0 MB)\n",
      "\n",
      "Export complete (17.5s)\n",
      "Results saved to \u001b[1m/student/vcheruku/Enhance-Data-Diversity-and-Robustness/yolo-for-single-class/YOLO11/yolo-on-single-object-detection/weights\u001b[0m\n",
      "Predict:         yolo predict task=segment model=/student/vcheruku/Enhance-Data-Diversity-and-Robustness/yolo-for-single-class/YOLO11/yolo-on-single-object-detection/weights/best.onnx imgsz=640  \n",
      "Validate:        yolo val task=segment model=/student/vcheruku/Enhance-Data-Diversity-and-Robustness/yolo-for-single-class/YOLO11/yolo-on-single-object-detection/weights/best.onnx imgsz=640 data=/student/vcheruku/Enhance-Data-Diversity-and-Robustness/config-yolo-single-class/data.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Model saved to: /student/vcheruku/Enhance-Data-Diversity-and-Robustness/yolo-for-single-class/YOLO11/yolo-on-single-object-detection/weights/quantization/best.onnx\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Paths\n",
    "custom_export_dir = '/student/vcheruku/Enhance-Data-Diversity-and-Robustness/yolo-for-single-class/YOLO11/yolo-on-single-object-detection/weights/quantization'\n",
    "os.makedirs(custom_export_dir, exist_ok=True)\n",
    "\n",
    "# Load trained model\n",
    "model = YOLO('/student/vcheruku/Enhance-Data-Diversity-and-Robustness/yolo-for-single-class/YOLO11/yolo-on-single-object-detection/weights/best.pt')\n",
    "\n",
    "# Export to ONNX\n",
    "onnx_path = model.export(format='onnx', dynamic=True, simplify=True)\n",
    "\n",
    "# Destination path in custom directory\n",
    "destination_path = os.path.join(custom_export_dir, 'best.onnx')\n",
    "\n",
    "# Ensure destination directory exists\n",
    "os.makedirs(custom_export_dir, exist_ok=True)\n",
    "\n",
    "# Move the ONNX model to the custom directory\n",
    "shutil.move(onnx_path, destination_path)  # Moving the file to avoid cross-filesystem issues\n",
    "\n",
    "print(f\"Model saved to: {destination_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "736e3819-794b-4545-a6b8-10c583273684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: onnx in /student/vcheruku/.local/lib/python3.12/site-packages (1.17.0)\n",
      "Requirement already satisfied: onnxruntime in /student/vcheruku/.local/lib/python3.12/site-packages (1.21.1)\n",
      "Requirement already satisfied: onnxruntime-tools in /student/vcheruku/.local/lib/python3.12/site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/lib64/python3.12/site-packages (from onnx) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /student/vcheruku/.local/lib/python3.12/site-packages (from onnx) (4.25.6)\n",
      "Requirement already satisfied: coloredlogs in /student/vcheruku/.local/lib/python3.12/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /student/vcheruku/.local/lib/python3.12/site-packages (from onnxruntime) (25.2.10)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3.12/site-packages (from onnxruntime) (23.2)\n",
      "Requirement already satisfied: sympy in /student/vcheruku/.local/lib/python3.12/site-packages (from onnxruntime) (1.13.1)\n",
      "Requirement already satisfied: psutil in /usr/lib64/python3.12/site-packages (from onnxruntime-tools) (5.9.8)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/lib/python3.12/site-packages (from onnxruntime-tools) (9.0.0)\n",
      "Requirement already satisfied: py3nvml in /student/vcheruku/.local/lib/python3.12/site-packages (from onnxruntime-tools) (0.2.7)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /student/vcheruku/.local/lib/python3.12/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: xmltodict in /student/vcheruku/.local/lib/python3.12/site-packages (from py3nvml->onnxruntime-tools) (0.14.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /student/vcheruku/.local/lib/python3.12/site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.12 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install onnx onnxruntime onnxruntime-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b476a542-f131-467c-93f2-21bf07597b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization complete! The quantized model is saved to: /student/vcheruku/Enhance-Data-Diversity-and-Robustness/yolo-for-single-class/YOLO11/yolo-on-single-object-detection/weights/quantization/best-quant.onnx\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "import os\n",
    "\n",
    "# Define target directory\n",
    "output_dir = '/student/vcheruku/Enhance-Data-Diversity-and-Robustness/yolo-for-single-class/YOLO11/yolo-on-single-object-detection/weights/quantization/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Set paths\n",
    "input_model = '/student/vcheruku/Enhance-Data-Diversity-and-Robustness/yolo-for-single-class/YOLO11/yolo-on-single-object-detection/weights/quantization/best.onnx'\n",
    "output_model = os.path.join(output_dir, 'best-quant.onnx')\n",
    "\n",
    "# Perform quantization\n",
    "quantize_dynamic(\n",
    "    model_input=input_model,\n",
    "    model_output=output_model,\n",
    "    weight_type=QuantType.QUInt8  # or QuantType.QUInt8\n",
    ")\n",
    "\n",
    "print(f\"Quantization complete! The quantized model is saved to: {output_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc644a0-fa09-4257-b47f-fb4ea54cb131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ONNX model: /student/vcheruku/Enhance-Data-Diversity-and-Robustness/yolo-for-single-class/YOLO11/yolo-on-single-object-detection/weights/quantization/best-quant.onnx\n",
      "Inference Time: 0.1088 seconds\n",
      "Boxes Shape: (8400, 4), Confidences Shape: (8400,), Labels Shape: (8400,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import onnxruntime as ort\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "onnx_model_path = '/student/vcheruku/Enhance-Data-Diversity-and-Robustness/yolo-for-single-class/YOLO11/yolo-on-single-object-detection/weights/quantization/best-quant.onnx'  # Path to the ONNX model\n",
    "image_path = '/student/vcheruku/Enhance-Data-Diversity-and-Robustness/yolo-for-single-class/images/test/video01_video01_28900_frame_28914_endo.png'  # Path to the test image\n",
    "\n",
    "# === IMAGE PREPROCESSING ===\n",
    "def preprocess_image(image_path, img_size=(640, 640)):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = image.resize(img_size)\n",
    "    img = np.array(image).astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
    "    img = img.transpose(2, 0, 1)  # Convert to CHW format (Channels, Height, Width)\n",
    "    return np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "\n",
    "# === NON-MAXIMUM SUPPRESSION ===\n",
    "def non_maximum_suppression(boxes, confidences, threshold=0.5):\n",
    "    boxes = boxes.reshape(-1, 4)\n",
    "    confidences = confidences.flatten()\n",
    "    indices = cv2.dnn.NMSBoxes(boxes.tolist(), confidences.tolist(), score_threshold=threshold, nms_threshold=0.4)\n",
    "    return indices.flatten() if indices is not None and len(indices) > 0 else []\n",
    "\n",
    "# === INFERENCE FUNCTION FOR ONNX MODEL ===\n",
    "def run_onnx_model(onnx_model_path, image_path):\n",
    "    session = ort.InferenceSession(onnx_model_path, providers=[\"CPUExecutionProvider\"])\n",
    "    img = preprocess_image(image_path)  # Preprocess the input image\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    start_time = time.time()\n",
    "    outputs = session.run(None, {input_name: img})\n",
    "    inference_time = time.time() - start_time\n",
    "    return outputs, inference_time\n",
    "\n",
    "# === VISUALIZE INFERENCE RESULTS ===\n",
    "def visualize_inference(image_path, boxes, confidences, labels):\n",
    "    image = cv2.imread(image_path)\n",
    "    for i in range(len(boxes)):\n",
    "        box = boxes[i]\n",
    "        x1, y1, x2, y2 = map(int, box)  # Ensure box has 4 values\n",
    "        confidence = float(confidences[i])\n",
    "        label = labels[i] if labels is not None else 0  # Use label 0 as fallback\n",
    "        color = (0, 255, 0)  # Green color for bounding boxes\n",
    "\n",
    "        # Draw rectangle\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "        # Add label and confidence score\n",
    "        cv2.putText(image, f\"Class {label} ({confidence:.2f})\", (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    # Show the image with bounding boxes\n",
    "    cv2.imshow(\"Inference Result\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# === RUN INFERENCE ON ONNX MODEL ===\n",
    "def test_onnx_model(onnx_model_path, image_path):\n",
    "    print(f\"Testing ONNX model: {onnx_model_path}\")\n",
    "\n",
    "    if not os.path.exists(onnx_model_path):\n",
    "        print(f\"Error: Model file not found at {onnx_model_path}\")\n",
    "        return\n",
    "\n",
    "    # Run inference on the image\n",
    "    outputs, inference_time = run_onnx_model(onnx_model_path, image_path)\n",
    "\n",
    "    print(f\"Inference Time: {inference_time:.4f} seconds\")\n",
    "\n",
    "    # Extract results for visualization (assuming result contains boxes, confidences, and labels)\n",
    "    boxes = outputs[0][:, :4].reshape(-1, 4)  # Flatten the boxes\n",
    "    confidences = outputs[0][:, 4].flatten()  # Flatten the confidence scores\n",
    "    labels = outputs[0][:, 5].flatten()  # Flatten the labels\n",
    "\n",
    "    # Ensure that boxes are properly shaped (4 values per box)\n",
    "    print(f\"Boxes Shape: {boxes.shape}, Confidences Shape: {confidences.shape}, Labels Shape: {labels.shape}\")\n",
    "\n",
    "    # Visualize the results\n",
    "    visualize_inference(image_path, boxes, confidences, labels)\n",
    "\n",
    "\n",
    "# === RUN THE TEST ===\n",
    "test_onnx_model(onnx_model_path, image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29477923-bbf6-412a-8b28-379108ef04c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
